<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns='http://www.w3.org/1999/xhtml'>
  <head>
    <meta http-equiv='content-type' content='text/html; charset=utf-8'/> 
    <title>Borran/Fatemi/Horstmann Scala Unit 19</title> 
    <link href='http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css' media='screen, projection, print' rel='stylesheet' type='text/css'/> 
    <link href='../MySlidy/style.css' media='screen, projection, print' rel='stylesheet' type='text/css'/> 
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <script type='text/javascript' charset='utf-8' src='http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js.gz'></script> 
   </head>
 
<body >  
    <h1>Programmation appliquée en Scala</h1> 
    <p class='fullimage'><img src='../images/cheseaux.jpg'/></p> 
    <div class='license'>
      <p>Copyright © Cay S. Horstmann 2015 <a rel='license' href='http://creativecommons.org/licenses/by/4.0/'><img alt='Creative Commons License' style='border-width:0' src='https://i.creativecommons.org/l/by/4.0/88x31.png'/></a><br/>This work is licensed under a <a rel='license' href='http://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 International License</a> </p> 
    </div> 

<h1 id="data-parallel-programming">Distributed Data-Parallel Programming with Spark</h1>
<p> <i>Note: The content of this unit is adapted from the slides of the course "Parallel Programming and Data Analysis" <br>
by Heather Miller at EPFL.</i></p>

<p>So far we have seen: </p>

<ul>
<li>Principles of programming in Scala in sequential and non-distributed style.</li>
<li>A number of useful tools and technologies based on Scala.</li>
</ul>
Scala also offers parallel programming via "parallel collections", allowing to use collections 
in a very similar way that we have learned so far but taking advantage of data parallelism, hence 
better exploiting multi-core/multi-processor architecture.   
<br>Although this aspect will not be treated in the current course.  


<p>What we'll see in this course:</p>

<ul>
<li>Data parallelism in a <em>distributed setting</em>.</li>
<li>Distributed collections abstraction from Apache Spark as an <br>
implementation of this paradigm.</li>
</ul>



<h1 id="distribution">Distribution</h1>

<p>Distribution introduces important concerns beyond what we have to <br>
worry about when dealing with parallelism in the shared memory case:</p>

<ul>
<li><em>Partial failure:</em> crash failures of a subset of the machines <br>
involved in a distributed computation.</li>
<li><em>Latency:</em> certain operations have a much higher latency than <br>
other operations due to network communication.</li>
</ul>

Latency cannot be masked completely; it will be an important aspect that also impacts the programming model.


<h1 id="data-parallel-to-distributed-data-parallel">Data-Parallel to <strong>Distributed</strong> Data-Parallel</h1>


<p>What does <strong>distributed</strong> data-parallel look like?</p>

<table>
  <tr> 
     <td width="500"> <image width="50%" height=auto src="./week19-07.jpg"></td>
    <td width="500"> <image width="50%" height= auto src="./week19-08.jpg"></td>
  </tr>
</table>

<p><strong>Shared memory case:</strong> Data-parallel programming model. Data partitioned in <br>
memory and operated upon in parallel.</p>

<p><strong>Distributed case:</strong> Data-parallel programming model. Data partitioned <br>
between  machines, network in between, operated upon in parallel.</p>


<h1 id="apache-spark">Apache Spark</h1>
<image src="spark-logo.png" align="right">
<p>Throughout this part of the course we will use the <b>Apache Spark</b>
  framework for distributed data-parallel programming.
<p>Spark implements a distributed data parallel model called <b>Resilient Distributed Datasets (RDDs)</b></p>




<h1 id="book">Book</h1>

<center><image src="learning-spark.jpg" width="25%" height="auto"/></center>

<center><p><em>Learning Spark</em> by Holden Karau, Andy Konwinski, Patrick Wendell &amp; <br>
Matei Zaharia. O’Reilly, February 2015.</p></center>

<h1 id="resilient-distributed-datasets-rdds">Resilient Distributed Datasets (RDDs)</h1>

<p>RDDs look just like <strong><em>immutable</em></strong> sequential or parallel Scala collections.</p>
<table>
<tr>
  <td width="10%"><b>Combinators on Scala <br>parallel/sequential collections:</b></td>
  <td width="20%"><b>Combinators on RDDs:</b></td>
</tr>
<tr>
  <td>map</td>
  <td>map</td>
</tr>
<tr>
  <td>flatMap</td>
  <td>flatMap</td>
</tr>
<tr>
  <td>filter</td>
  <td>filter</td>
</tr>
<tr>
  <td>reduce</td>
  <td>reduce</td>
</tr>
<tr>
  <td>fold</td>
  <td>fold</td>
</tr>
<tr>
  <td>aggregate</td>
  <td>aggregate</td>
</tr>
</table>


<h1 id="resilient-distributed-datasets-rdds-1">Resilient Distributed Datasets (RDDs)</h1>

<p>While their signatures differ a bit, their semantics (macroscopically) are the <br>
same:</p>

<pre><code>map[B](f: A =&gt; B): List[B] // Scala List
map[B](f: A =&gt; B): RDD[B] // Spark RDD

flatMap[B](f: A =&gt; TraversableOnce[B]): List[B] // Scala List
flatMap[B](f: A =&gt; TraversableOnce[B]): RDD[B] // Spark RDD

filter(pred: A =&gt; Boolean): List[A] // Scala List
filter(pred: A =&gt; Boolean): RDD[A] // Spark RDD
</code></pre>



<h1 id="resilient-distributed-datasets-rdds-2">Resilient Distributed Datasets (RDDs)</h1>

<p>While their signatures differ a bit, their semantics (macroscopically) are the <br>
same:</p>

<pre><code>reduce(op: (A, A) =&gt; A): A // Scala List
reduce(op: (A, A) =&gt; A): A // Spark RDD

fold(z: A)(op: (A, A) =&gt; A): A // Scala List
fold(z: A)(op: (A, A) =&gt; A): A // Spark RDD

aggregate[B](z: =&gt; B)(seqop: (B, A) =&gt; B, combop: (B, B) =&gt; B): B // Scala
aggregate[B](z: B)(seqop: (B, A) =&gt; B, combop: (B, B) =&gt; B): B // Spark RDD
</code></pre>



<h1 id="resilient-distributed-datasets-rdds-3">Resilient Distributed Datasets (RDDs)</h1>

<p>Using RDDs in Spark feels a lot like normal Scala sequential/parallel <br>
collections, with the added knowledge that your data is distributed across <br>
several machines.</p>

<p><b>Example:</b></p>

<p>Given, <code>val encyclopedia: RDD[String]</code>, say we want to search <br>
all of <code>encyclopedia</code> for mentions of HEIG-VD, and count the number of pages that <br>
mention HEIG-VD.</p>

<pre><code>val result = encyclopedia.filter(page =&gt; page.contains("HEIG-VD"))
                         .count()
</code></pre>

<h1 id="example-word-count">Example: Word Count</h1>

<p>The “Hello, World!” of programming with large-scale data.</p>

<pre><code>// Create an RDD
val rdd = spark.textFile("hdfs://...")

val count = ???
</code></pre>



<h1 id="example-word-count-1">Example: Word Count</h1>

<p>The “Hello, World!” of programming with large-scale data.</p>

<pre><code>// Create an RDD
val rdd = spark.textFile("hdfs://...")

val count = rdd.flatMap(line =&gt; line.split(" ")) // separate lines into words
</code></pre>



<h1 id="example-word-count-2">Example: Word Count</h1>

<p>The “Hello, World!” of programming with large-scale data.</p>

<pre><code>// Create an RDD
val rdd = spark.textFile("hdfs://...")

val count = rdd.flatMap(line =&gt; line.split(" ")) // separate lines into words
               .map(word =&gt; (word, 1))           // include something to count
</code></pre>



<h1 id="example-word-count-3">Example: Word Count</h1>

<p>The “Hello, World!” of programming with large-scale data.</p>

<pre><code>// Create an RDD
val rdd = spark.textFile("hdfs://...")

val count = rdd.flatMap(line =&gt; line.split(" ")) // separate lines into words
               .map(word =&gt; (word, 1))           // include something to count
               .reduceByKey(_ + _)               // sum up the 1s in the pairs
</code></pre>

<p>That’s it.</p>



<h1 id="transformations-and-actions">Transformations and Actions</h1>

<p>Collection methodes that you have already seen in the Scala standard library <br>are divided into two major groupes:</p>
<p><b>Transformers: </b>  Return new collections as results. (Not single values.) </p>
<p><b>Examples:</b> <code>map, filter, flatMap, groupBy</code><br> </p>
<code><pre>map(f: A => B): Traversable[B] </code></pre>


<p><b>Accessors:</b> Return single values as results. (Not collections.) <br></p>
<p><b>Examples:</b> <code>reduce, fold, aggregate</code><br></p>
<code><pre>reduce(op: (A, A) => A): A</pre></code>



<h1 id="transformations-and-actions-1">Transformations and Actions</h1>

<p>Similarly, Spark defines <em><strong>transformations</strong></em> and <em><strong>actions</strong></em> on RDDs.</p>

<p>They seem similar to transformers and accessors, but there are some <br>
important differences.</p>


<p><b>Transformations.</b> Return new <b>RDDs</b> as results. <br></p>

<p><b>Actions.</b> Compute a result based on an RDD, and either <br>
returned or saved to an external storage system (e.g., HDFS). <br></p>

<h1 id="transformations-and-actions-1">Transformations and Actions</h1>
<p>Transformations are <i>lazy</i>, their result RDD is not immediately computed.</p>
<p>They are <i>eager</i>, their result is immediately computed.</p>
<image src="week19-10.jpg" width="50%" height="auto">

<h1 id="exercise">Quiz</h1>

<p>Consider the following simple example:</p>

<pre><code>val largeList: List[String] = ...
val wordsRdd = sc.parallelize(largeList)
val lengthsRdd = wordsRdd.map(_.length)
</code></pre>

<p>What has happened on the cluster at this point?</p>

	<ol class='clicker'>
      <li>The lengths of the strings in the RDD are calculated on the worker nodes</li> 
      <li>The lengths of the strings in the RDD are calculated on the worker nodes and returned to the master node</li> 
      <li>The sum of the lengths of the strings in the RDD is calculated by the master node </li> 
      <li>Nothing</li> 
    </ol>



<h1 id="example-1">Example</h1>

<p>Consider the following simple example:</p>

<pre><code>val largeList: List[String] = ...
val wordsRdd = sc.parallelize(largeList)
val lengthsRdd = wordsRdd.map(_.length)
val totalChars = lengthsRdd.reduce(_ + _)
</code></pre>

<p><strong>... we can add an action!</strong></p>



<h1 id="cluster-topology-matters">Cluster Topology Matters</h1>

<p><strong>If you perform an action on an RDD, on what machine is its result “returned” to?</strong></p>

<p>
<b>Example:</b></p>

<pre><code>val people: RDD[Person] = ...
val first10 = people.take(10)
</code></pre>

<p>Where will the <code>Array[Person]</code> representing <code>first10</code> end up?</p>


<h1 id="execution-of-spark-programs">Execution of Spark Programs</h1>

<p>A Spark application is run using a set of processes on a cluster. All these processes are <br>
coordinated by the <em><strong>driver program</strong></em>.</p>

<center><image src="week19-11.jpg" width="30%" height="auto"></center>


<ol>
<li>The driver program runs the Spark application, which creates a <code>SparkContext</code> upon start-up.</li>
<li>The <code>SparkContext</code> connects to a cluster manager (e.g., Mesos/YARN) which allocates resources.</li>
<li>Spark acquires <em><strong>executors</strong></em> on nodes in the cluster, which are processes that run computations and store data for your application.</li>
<li>Next, driver program sends your application code to the executors.</li>
<li>Finally, <code>SparkContext</code> sends <em><strong>tasks</strong></em> for the executors to run.</li>
</ol>



<h1 id="cluster-topology-matters-1">Cluster Topology Matters</h1>

<p><strong>If you perform an action on an RDD, on what machine is its result “returned” to?</strong></p>

<p><b>Example</b></p>

<pre><code>val people: RDD[Person] = ...
val first10 = people.take(10)
</code></pre>

<p>Where will the <code>Array[Person]</code> representing <code>first10</code> end up?</p>


<p><strong>The driver program.</strong></p>

<p><i>In general, executing an action involves communication between worker nodes <br>
and the node running the driver program.</i></p>



<h1 id="benefits-of-laziness-for-large-scale-data">Benefits of Laziness for Large-Scale Data</h1>

<p>Spark computes RDDs the first time they are used in an action.</p>

<p>This helps when processing large amounts of data.</p>

<p><b>Example:</b></p>

<pre><code>val lastYearsLogs: RDD[String] = ...
val firstLogsWithErrors = lastYearsLogs.filter(_.contains("ERROR")).take(10)
</code></pre>

<p><i>In the above example, lazy execution of `filter` makes a big difference.</i></p>

<p>The execution of <code>filter</code> is deferred until the <code>take</code> action is applied.</p>

<p>Spark leverages this by analyzing and optimizing the <strong>chain of operations</strong> before executing it.</p>

<p>Spark will not compute intermediate RDDs. Instead, as soon as 10 elements of the filtered RDD have been computed, <code>firstLogsWithErrors</code> is done. At this point Spark stops working, saving time and space computing elements of the unused result of <code>filter</code>.</p>



<h1 id="caching-and-persistence">Caching and Persistence</h1>

<p>By default, RDDs are recomputed each time you run an action on them. This can <br>
be expensive (in time) if you need to traverse a dataset more than once.</p>



<p><b>Spark allows you to control what is cached in memory.</b></p>

<p><code><pre>
val lastYearsLogs: RDD[String] = ...
val logsWithErrors = lastYearsLogs.filter(_.contains("ERROR")).persist()
val firstLogsWithErrors = logsWithErrors.take(10)
</pre></code></p>

<p>Here, we <em>cache</em> <code>logsWithErrors</code> in memory.</p>

<p>After <code>firstLogsWithErrors</code> is computed, Spark will store the contents of <br>
<code>logsWithErrors</code> for faster access in future operations if we would like to <br>
reuse it.</p>

<p><code><pre>val numErrors = logsWithErrors.count() // faster
</pre></code></p>

<p><strong>Now, computing the <code>count</code> on <code>logsWithErrors</code> is much faster.</strong></p>



<h1 id="caching-and-persistence-2">Caching and Persistence</h1>

<p><strong>Persistence levels.</strong> Other ways to control how Spark stores objects.</p>

<img src="sparkPersistenceLevels.png"/>


<h1 id="other-important-rdd-transformations">Other Important RDD Transformations</h1>

<p>Beyond the transformer-like combinators you may be familiar with from Scala <br>
collections, RDDs introduce a number of other important transformations.</p>

<table>
<tr>
  <td width="20">sample</td>
  <td width="10">Sample a fraction fraction of the data, with or without
replacement, using a given random number generator seed.</td>
</tr>
<tr>
  <td>union</td>
  <td>Return a new dataset that contains the union of the elements in the source dataset and the argument. Pseudo-set operations (duplicates remain).</td>
</tr>
<tr>
  <td>intersection</td>
  <td>Return a new RDD that contains the intersection of elements in the source dataset and the argument. Pseudo-set operations (duplicates remain).</td>
</tr>
</table>



<h1 id="other-important-rdd-transformations-2">Other Important RDD Transformations (2)</h1>

<table>
<tr>
  <td width="20">distinct</td>
  <td width="10">Return a new dataset that contains the distinct elements of the source dataset.</td>
</tr>
<tr>
  <td>coalesce</td>
  <td>Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.</td>
</tr>
<tr>
  <td>repartition</td>
  <td>Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. </td>
</tr>
</table>


<h1 id="other-important-rdd-actions">Other Important RDD Actions</h1>

<p>RDDs also contain other important actions which are useful when dealing with distributed data.</p>
<table>
<tr>
  <td width="20">collect</td>
  <td width="10">Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</td>
</tr>
<tr>
  <td>count</td>
  <td>Return the number of elements in the dataset.</td>
</tr>
<tr>
  <td>foreach</td>
  <td>Run a function func on each element of the dataset. This is usually done for side effects such as interacting with external storage systems.</td>
</tr>
<tr>
  <td>saveAsTextFile</td>
  <td>Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.</td>
</tr>
</table>


<h1 id="pair-rdds">Pair RDDs</h1>

<p>Often when working with distributed data, it’s useful to organize data into <br>
<strong>key-value pairs</strong>. In Spark, these are Pair RDDs.</p>

<p><strong>Useful because:</strong> Pair RDDs allow you to act on each key in parallel or regroup <br>
data across the network.</p>

<p>Spark provides powerful extension methods for RDDs containing pairs (<em>e.g.,</em> <br>
<code>RDD[(K, V)]</code>). Some of the most important extension methods are:</p>

<pre><code>def groupByKey(): RDD[(K, Iterable[V])]
def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]
def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
</code></pre>

<p>Depending on the operation, data in an RDD may have to be <strong>shuffled</strong> among <br>
worker nodes, using worker-worker communication.</p>

<p><b>This is often the case for many operations Pair RDDs!</b></p>

<!-- UP TO HERE CHNAGED -->















<h1>Lab: Spark Setup</h1>
<p>First download Spark form <a href="http://www.apache.org/dyn/closer.cgi/spark/spark-1.3.1/spark-1.3.1-bin-hadoop2.6.tgz">here</a>.
And then open the archive and goto inside Spark directory and run</p>
<pre><code>./bin/run-example SparkPi 10
</code></pre>
<p>It should compute Pi without any error message.</p>
<p>Spark runs on Java 6+ and Python 2.6+. 
For the Scala API, Spark 1.3.1 uses Scala 2.10. 
You will need to use a compatible Scala version (2.10.x).
The additional information can be found on
<a href="https://spark.apache.org/docs/latest/">Spark documentation</a>.</p>

<h1>Part1: Interactive Analysis with the Spark Shell</h1>
<p>We use Spark shell to learn the framework.
Run following command from Spark home directory:</p>
<pre><code>./bin/spark-shell
</code></pre>
<p>We are going to process <code>README.md</code> file of Spark.
Run following commands on Spark shell and explain each line and show the results:</p>
<pre><code>scala&gt; val textFile = sc.textFile("README.md")

scala&gt; textFile.count

scala&gt; textFile.first

scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains("Spark"))

scala&gt; textFile.filter(line =&gt; line.contains("Spark")).count
</code></pre>

<h1>Part2: Self-Contained Applications</h1>
<p>Now we create a very simple Spark application in Scala.</p>
<pre><code>/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
    def main(args: Array[String]) {
        val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
        val conf = new SparkConf().setAppName("Simple Application")
        val sc = new SparkContext(conf)
        val logData = sc.textFile(logFile, 2).cache()
        val numAs = logData.filter(line =&gt; line.contains("a")).count()
        val numBs = logData.filter(line =&gt; line.contains("b")).count()
        println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
    }
}
</code></pre>
<p>Note that applications should define a <code>main()</code> method instead of extending <code>scala.App</code>. 
Subclasses of <code>scala.App</code> may not work correctly.
What does this program?</p>
<p>Our application depends on the Spark API, so we'll also include an <code>sbt</code> configuration file, 
<code>build.sbt</code> which explains that Spark is a dependency. This file also adds a repository that Spark depends on:</p>
<pre><code>name := "Simple Project"

version := "1.0"

scalaVersion := "2.10.4"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.3.1"
</code></pre>
<p>For sbt to work correctly, we'll need to layout <code>SimpleApp.scala</code> and <code>build.sbt</code> according 
to the typical directory structure. Once that is in place, we can create a JAR package 
containing the application's code, then use the <code>spark-submit</code> script to run our program.</p>
<pre><code># Your directory layout should look like this
$ find .
.
./build.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala

# Package a jar containing your application
$ activator package
...
[info] Packaging {..}/{..}/target/scala-2.10/simple-project_2.10-1.0.jar

# Use spark-submit to run your application
$ YOUR_SPARK_HOME/bin/spark-submit \
--class "SimpleApp" \
--master local[4] \
target/scala-2.10/simple-project_2.10-1.0.jar
...
</code></pre>
<p>What is the output of the program?</p>


<h1>Homework: Exploring full-text Wikipedia articles</h1>
<p>In this assignment, you will get to know Spark by exploring full-text Wikipedia articles.
To start first download the assignment: <a href="./wikipedia.zip">wikipedia.zip</a></p>
<p>Gauging how popular a programming language is important for companies judging whether or 
not they should adopt an emerging programming language. For that reason, industry analyst 
firm RedMonk has bi-annually computed a ranking of programming language popularity using 
a variety of data sources, typically from websites like GitHub and StackOverflow. 
See their <a href="http://redmonk.com/sogrady/2015/01/14/language-rankings-1-15/">top-20 ranking for January 2015</a> as an example.</p>
<p>In this assignment, we'll use our full-text data from Wikipedia to produce a rudimentary 
metric of how popular a programming language is, in an effort to see if our Wikipedia-based 
rankings bear any relation to the popular RedMonk rankings.
You'll complete this exercise on just one node (your laptop).</p>
<p>First create a <code>SparkConfig</code> instance. A <code>SparkConfig</code> represents the configuration of your Spark application. 
It's here that you must specify that you intend to run your application in "local" mode. 
You must also name your Spark application at this point.
Then create a <code>SparkContext</code>. A <code>SparkContext</code> is the "handle" to your cluster. 
Once you have a <code>SparkContext</code>, you can use it to create and populate <code>RDD</code>s with data.
For help, see the <a href="https://spark.apache.org/docs/latest/api/scala/index.html#package">Spark API Docs</a>.</p>

<h1>Read-in Wikipedia Data</h1>
<p>There are several ways to read data into Spark. The simplest (but most unrealistic) way 
to read in data is to convert an existing collection in memory to an RDD using the <code>parallelize</code> 
method of the Spark context.</p>
<p>We have already pre-processed a small chunk of Wikipedia for you, and have made it available 
in the <code>articles</code> member of the <code>WikipediaData</code> object.
Create an <code>RDD</code> (by implementing <code>val wikiRdd</code>) which contains the <code>WikipediaArticle</code> objects of <code>articles</code>.</p>

<h1>Compute a ranking of programming languages</h1>
<p>We will use a simple metric for determining the popularity of a programming language: 
the number of Wikipedia articles that mention the language at least once.</p>

<h1>Rank languages attempt #1: <code>rankLangs</code></h1>
<p>Start by implementing a helper method <code>occurrencesOfLang</code> which computes the number of 
articles in an RDD of type <code>RDD[WikipediaArticles]</code> that mention the given language at least once.</p>
<p>Computing the ranking, <code>rankLangs</code> using <code>occurrencesOfLang</code>, implement a method <code>rankLangs</code> 
which computes a list of pairs where the second component of the pair is the number of 
articles that mention the language (the first component of the pair is the name of the language).</p>
<p>An example of what <code>rankLangs</code> returns might look like this:</p>
<pre><code>List(("Scala",999999),("JavaScript",1278),("LOLCODE",982),("Java",42))
</code></pre>
<p>The list should be sorted in descending order. That is, according to this ranking, the pair 
with the highest second component (the count) should be the first element of the list.</p>
<p>Pay attention to roughly how long it takes to run this part! (It should take tens of seconds.)</p>
<p>How long does the code take? What is the list of ranked languages?</p>

<h1>Rank languages attempt #2: <code>rankLangsUsingIndex</code></h1>

<p><strong>Compute an inverted index: <code>makeIndex</code></strong></p>
<p>An inverted index is an index data structure storing a mapping from content, 
such as words or numbers, to a set of documents. In particular, the purpose of an inverted 
index is to allow fast full text searches. In our use-case, an inverted index would be useful 
for mapping from the names of programming languages to the collection of Wikipedia articles
that mention the name at least once.</p>
<p>To make working with the dataset more efficient and more convenient, implement a method 
that computes an "inverted index" which maps programming language names to the Wikipedia 
articles on which they occur at least once.</p>
<p>Implement method <code>makeIndex</code> which returns an RDD of the following type: <code>RDD[(String, Iterable[WikipediaArticle])]</code>. 
This RDD contains pairs, such that for each language in the given <code>langs</code> list there is at most one pair. 
Furthermore, the second component of each pair (the <code>Iterable</code>) contains the <code>WikipediaArticles</code>
that mention the language at least once. </p>
<p><em>Hint:</em> You might want to use methods <code>flatMap</code> and <code>groupByKey</code> on RDD for this part.</p>

<p><strong>Computing the ranking: <code>rankLangsUsingIndex</code></strong></p>
<p>Use the <code>makeIndex</code> method implemented in the previous part to implement a faster method 
for computing the language ranking.</p>
<p>Like in part 1, <code>rankLangsUsingIndex</code> should compute a list of pairs where the second 
component of the pair is the number of articles that mention the language 
(the first component of the pair is the name of the language).</p>
<p>Again, the list should be sorted in descending order. That is, according to this ranking, 
the pair with the highest second component (the count) should be the first element of the list.</p>
<p>How long does the code take? What is the list of ranked languages?</p>
<p>Can you notice a performance improvement over attempt #1? Why?</p>

<h1>Rank languages attempt #3: <code>rankLangsReduceByKey</code></h1>
<p>In the case where the inverted index from above is only used for computing the ranking 
and for no other task (full-text search, say), it is more efficient to use the <code>reduceByKey</code> 
method to compute the ranking directly, without first computing an inverted index. 
Note that the <code>reduceByKey</code> method is only defined for RDDs containing pairs 
(each pair is interpreted as a key-value pair).</p>
<p>Implement the <code>rankLangsReduceByKey</code> method, this time computing the ranking without 
the inverted index, using <code>reduceByKey</code>.</p>
<p>Like in part 1 and 2, <code>rankLangsReduceByKey</code> should compute a list of pairs where the 
second component of the pair is the number of articles that mention the language 
(the first component of the pair is the name of the language).</p>
<p>Again, the list should be sorted in descending order. That is, according to this ranking, 
the pair with the highest second component (the count) should be the first element of the list.</p>
<p><em>Hint:</em> method <code>mapValues</code> on <code>PairRDD</code> could be useful for this part.</p>
<p>How long does the code take? What is the list of ranked languages?</p>
<p>Can you notice an improvement in performance compared to measuring both the computation of
the index and the computation of the ranking as we did in attempt #2? If so, can you think of a reason?</p>

      
    </body>
  </html>
